https://arxiv.org/abs/2010.12237

https://github.com/google-deepmind/deepmind-research/tree/master/causal_reasoning

This paper introduces concrete algorithms for causal reasoning in discrete probability trees, which are models for representing the causal generative process of a random experiment or stochastic process. Key points:

Probability trees encode the potential states of a process as nodes, with probabilistic transitions and causal dependencies represented by arrows between nodes. Unlike causal Bayesian networks (3CBNs), they can model context-specific causal dependencies.
The paper provides algorithms for:


Computing minimal representations (min-cuts) of arbitrary events formed through propositional calculus and causal precedence relations.
Computing the three fundamental operations of the causal hierarchy - conditions, interventions, and counterfactuals - on these events.


Events are represented as min-cuts that collect the minimal set of nodes in the tree that resolve whether the event has occurred or not. Min-cuts for simple events, negations, conjunctions, disjunctions and causal precedence are computed recursively.
Conditioning updates the transition probabilities after an event is revealed to be true. Computationally, it filters the tree to remove mass from realizations incompatible with the event.
Intervening minimally changes transition probabilities to force an event to occur. It affects only realizations downstream of a critical set of nodes.
Counterfactuals spawn a new variable scope from a factual premise tree modified by an intervention. This resets variables downstream of the intervention to their original state.

In summary, the paper expands the domain of causal reasoning to the very general class of discrete probability trees by providing interpretation-agnostic recursive algorithms for event representation and the key operations of seeing, doing and imagining. The treatment generalizes key concepts from the literature on CBNs and structural causal models.