https://proceedings.neurips.cc/paper/2021/hash/c3e0c62ee91db8dc7382bde7419bb573-Abstract.html

https://github.com/google-deepmind/deepmind-research/tree/master/tandem_dqn

This paper proposes the "tandem learning" experimental paradigm to study the challenges of learning to act from observational data without active environmental interaction (i.e., offline reinforcement learning) in the context of deep reinforcement learning. Key points:

Tandem learning involves two agents - an active agent that interacts with the environment to generate data, and a passive agent that learns from the data generated by the active agent without interacting with the environment itself.
The "tandem effect" refers to the consistent failure of the passive agent to learn as effectively as the active agent, despite having access to the same data. This effect is demonstrated across various environments and agent architectures.
Three hypothesized factors contributing to the tandem effect are investigated:


Bootstrapping on poorly estimated values
Insufficient coverage of sub-optimal actions in the active agent's data distribution
Extrapolation errors by the passive agent's function approximator


Experiments indicate that while bootstrapping plays an amplifying role, it is not the fundamental cause. The interplay between the data distribution and function approximation seems most critical.
Increasing the stochasticity of the active agent's policy, replay buffer size, and small amounts of the passive agent's self-generated data help mitigate but not eliminate the tandem effect.
The function approximator's capacity and tendency to extrapolate from limited data are found to strongly influence the magnitude of the tandem effect.
Overall, the results highlight the critical role of interactivity in learning control policies, with inadequate data coverage and extrapolation errors by the function approximator being the key challenges in offline learning.

The authors propose that the tandem learning paradigm, by decoupling learning dynamics from data generation, can serve as a useful analytic tool to study reinforcement learning algorithms in general.