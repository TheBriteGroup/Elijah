https://arxiv.org/abs/2002.09405

https://github.com/google-deepmind/deepmind-research/tree/master/learning_to_simulate

This paper introduces Graph Network-based Simulators (GNS), a framework for learning to simulate complex physical systems from data using graph neural networks. Key points:

GNS represents a physical system as a graph, with nodes corresponding to particles and edges to their interactions. The system's dynamics are approximated by learned message-passing among the nodes.
The model architecture follows an encode-process-decode paradigm:


The encoder maps the input particle state into a latent graph.
The processor performs multiple rounds of learned message-passing on the latent graph to propagate interactions.
The decoder extracts the dynamics information like accelerations from the final latent graph.


GNS can learn to accurately simulate fluids, rigid solids, and deformable materials interacting with each other, from thousands of particles over thousands of time steps, using a single architecture.
It generalizes well to much larger systems and longer time scales than those seen during training. Performance was robust to most architectural choices.
Key factors for good performance were the model's ability to propagate long-range interactions via multiple message-passing steps, imposing an inductive bias for spatial invariance, and training procedures to mitigate accumulation of error over long rollouts.
In comparisons, GNS outperformed other state-of-the-art particle-based and mesh-based simulation approaches, being more accurate and general.

In summary, GNS leverages the strong relational inductive biases of graph neural networks to learn generalizable, accurate and efficient simulators for complex physical dynamics from particle-based data. The learned simulator's differentiability can enable a wide range of downstream applications.